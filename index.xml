<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>NTU-NLP on NTU-NLP</title>
    <link>https://ntunlpsg.github.io/</link>
    <description>Recent content in NTU-NLP on NTU-NLP</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>ntunlp &amp;copy; 2020</copyright>
    <lastBuildDate>Thu, 31 Mar 2022 00:00:00 +0000</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Recent Posts</title>
      <link>https://ntunlpsg.github.io/post/post/</link>
      <pubDate>Mon, 10 Sep 2018 00:00:00 +0800</pubDate>
      
      <guid>https://ntunlpsg.github.io/post/post/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Resources</title>
      <link>https://ntunlpsg.github.io/resources/list/</link>
      <pubDate>Wed, 20 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>https://ntunlpsg.github.io/resources/list/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Chart-to-Text: A Large-Scale Benchmark for Chart Summarization</title>
      <link>https://ntunlpsg.github.io/publication/chart2text/</link>
      <pubDate>Thu, 31 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>https://ntunlpsg.github.io/publication/chart2text/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Rethinking Self-Supervision Objectives for Generalizable Coherence Modeling</title>
      <link>https://ntunlpsg.github.io/publication/coherence_paradigm/</link>
      <pubDate>Thu, 31 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>https://ntunlpsg.github.io/publication/coherence_paradigm/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Straight to the Gradient: Learning to Use Novel Tokens for Neural Text Generation</title>
      <link>https://ntunlpsg.github.io/project/scalegrad/</link>
      <pubDate>Wed, 30 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>https://ntunlpsg.github.io/project/scalegrad/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;https://github.com/shawnlimn/ScaleGrad&#34; target=&#34;_blank&#34;&gt;github&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Source code for &lt;a href=&#34;http://proceedings.mlr.press/v139/lin21b/lin21b.pdf&#34; target=&#34;_blank&#34;&gt;Straight to the Gradient: Learning to Use Novel Tokens for Neural Text Generation&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Xiang Lin, Simeng Han and Shafiq Joty&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Accepted at 38th International Conference on Machine Learning (&lt;strong&gt;ICML&amp;rsquo;21&lt;/strong&gt;).&lt;/p&gt;

&lt;p&gt;The repo is adapted from &lt;a href=&#34;https://github.com/facebookresearch/unlikelihood_training/&#34; target=&#34;_blank&#34;&gt;Neural Unlikelihood Training&lt;/a&gt;. You could either follow the original repo or the instruction below to finish installing the dependencies.&lt;/p&gt;

&lt;h2 id=&#34;setup&#34;&gt;Setup&lt;/h2&gt;

&lt;h3 id=&#34;dependencies&#34;&gt;Dependencies&lt;/h3&gt;

&lt;p&gt;The implementation is a custom &lt;a href=&#34;https://github.com/pytorch/fairseq&#34; target=&#34;_blank&#34;&gt;fairseq&lt;/a&gt; module. Download and install fairseq:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/pytorch/fairseq.git
cd fairseq
pip install --editable .
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Install other dependencies:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install nltk
pip install pytorch-transformers  
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;installing-the-a-customised-module-for-scaegrad&#34;&gt;&amp;lsquo;Installing&amp;rsquo; the a customised module for ScaeGrad&lt;/h3&gt;

&lt;p&gt;Copy the &lt;code&gt;custom&lt;/code&gt; directory in this repo into the &lt;code&gt;fairseq&lt;/code&gt; repo that you downloaded above:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export FAIRSEQ_DIR=/path/to/fairseq
export SCALEGRAD_DIR=/path/to/ScaleGrad

cp -r $SCALEGRAD_DIR/custom $FAIRSEQ_DIR/fairseq
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;finetuning-gpt-2-for-language-modeling-and-auto-completion&#34;&gt;Finetuning GPT-2 for Language Modeling and Auto Completion&lt;/h2&gt;

&lt;p&gt;We assume that you are in the &lt;code&gt;fairseq&lt;/code&gt; base directory.&lt;/p&gt;

&lt;h3 id=&#34;dataset&#34;&gt;Dataset&lt;/h3&gt;

&lt;p&gt;Download and unpack the BPE-tokenized WikiText:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;wget https://dl.fbaipublicfiles.com/unlikelihood/wikitext-103-bpe_v0.tar.gz
tar -xzvf wikitext-103-bpe_v0.tar.gz
mv wikitext-103-bpe_v0 data-bin/
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;finetuning-with-scalegrad-mle&#34;&gt;Finetuning with ScaleGrad/MLE:&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;gamma&lt;/code&gt; is the only hyper-parameter needed to be pre-determined. &lt;code&gt;gamma=1&lt;/code&gt; is equivalent to performing MLE finetuning. In our paper, we experimented with three different choices from {0.2,0.5,0.8}.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python fairseq/custom/gpt2/run_gpt2.py  \
--data-base ./data-bin/wikitext-103-bpe_v0    \
--output-dir ./checkpoint/gpt-2   \
--eval-split valid    \
--train-n-steps 35000   \
--validate-every 1000    \
--mode train \
--train-batch-size 300    \
--gamma 0.2 \
--learning-rate 2e-5 \
--seed 60 
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;evaluation&#34;&gt;Evaluation&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;python fairseq/custom/gpt2/run_gpt2.py  \
    --data-base ./data-bin/wikitext-103-bpe_v0 \
    --output-dir ./checkpoint/gpt2/sg_output \
    --eval-split test \
    --model-load-dir ./checkpoint/gpt2/sg/best \
    --mode eval-both
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We used a single GTX 2080Ti gpu.&lt;/p&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;Please cite our work if you found the resources in this repository useful:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;@InProceedings{lin-2021-straight,
  title = 	 {Straight to the Gradient: Learning to Use Novel Tokens for Neural Text Generation},
  author =       {Lin, Xiang and Han, Simeng and Joty, Shafiq},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {6642--6653},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/lin21b/lin21b.pdf},
  url = 	 {http://proceedings.mlr.press/v139/lin21b.html},
  abstract = 	 {}
}


&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>LFPT5: A Unified Framework for Lifelong Few-shot Language Learning Based on Prompt Tuning of T5 </title>
      <link>https://ntunlpsg.github.io/project/lfpt5/</link>
      <pubDate>Thu, 10 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>https://ntunlpsg.github.io/project/lfpt5/</guid>
      <description>

&lt;p&gt;The repo is the source code for &lt;a href=&#34;https://openreview.net/forum?id=HCRVf71PMF&#34; target=&#34;_blank&#34;&gt;LFPT5: A Unified Framework for Lifelong Few-shot Language Learning Based on Prompt Tuning of T5&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;ndash;Chengwei Qin, Shafiq Joty&lt;/p&gt;

&lt;p&gt;Accepted at 10th  International Conference on Learning Representations (ICLR&amp;rsquo;22).&lt;/p&gt;

&lt;h2 id=&#34;setup&#34;&gt;Setup&lt;/h2&gt;

&lt;h3 id=&#34;1-download-the-code&#34;&gt;1. Download the code&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;git clone git@github.com:qcwthu/Lifelong-Fewshot-Language-Learning.git
cd Lifelong-Fewshot-Language-Learning
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;2-install-dependencies&#34;&gt;2. Install dependencies&lt;/h3&gt;

&lt;h4 id=&#34;2-1-create-conda-environment&#34;&gt;2.1. Create conda environment&lt;/h4&gt;

&lt;p&gt;For NER &amp;amp; Classification:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;conda create --name lfll_1 python=3.9.4
conda activate lfll_1
pip install gpustat
pip install tensorflow==2.5.0
pip install seqeval==1.2.2
pip install matplotlib==3.3.4
pip install numpy==1.19.5
pip install torch==1.7.1+cu110 -f https://download.pytorch.org/whl/torch_stable.html
cd transformers; pip install .; cd ..
pip install fairscale==0.3.7
pip install datasets==1.11.0
pip install sentencepiece==0.1.95
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For Summarization &amp;amp; Different task types:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;conda create --name lfll_2 python=3.9.4
conda activate lfll_2
conda instal matplotlib=3.5.0
pip install scikit-learn==1.0.2
pip install gpustat
pip install tensorflow==2.5.0
pip install seqeval==1.2.2
pip install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html
cd transformers; pip install .; cd ..
pip install fairscale==0.4.4
pip install datasets==1.17.0
pip install sentencepiece==0.1.96
pip install more_itertools==8.12.0
pip install rouge-score==0.0.4
pip install rouge==1.0.1
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;2-2-download-lm-adapted-t5-model&#34;&gt;2.2. Download LM-adapted T5 model&lt;/h4&gt;

&lt;p&gt;To run lifelong few-shot language learning with prompt tuning, you should download the LM-adapted T5 model from &lt;a href=&#34;https://github.com/google-research/text-to-text-transfer-transformer/blob/main/released_checkpoints.md#lm-adapted-t511lm100k&#34; target=&#34;_blank&#34;&gt;LM-Adapted: t5.1.1.lm100k&lt;/a&gt;. Then you should use &lt;code&gt;convertmodel.py&lt;/code&gt; to convert Tensorflow checkpoint to Pytorch checkpoint.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mkdir lm_adapted_t5model
cd lm_adapted_t5model
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then you should follow &lt;a href=&#34;https://cloud.google.com/storage/docs/gsutil_install&#34; target=&#34;_blank&#34;&gt;gsutil_install&lt;/a&gt; to install gsutil.&lt;/p&gt;

&lt;p&gt;After installing gsutil, run&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;gsutil -m cp \
  &amp;quot;gs://t5-data/pretrained_models/t5.1.1.lm100k.large/checkpoint&amp;quot; \
  &amp;quot;gs://t5-data/pretrained_models/t5.1.1.lm100k.large/model-info.txt&amp;quot; \
  &amp;quot;gs://t5-data/pretrained_models/t5.1.1.lm100k.large/model.ckpt-1100000.data-00000-of-00004&amp;quot; \
  &amp;quot;gs://t5-data/pretrained_models/t5.1.1.lm100k.large/model.ckpt-1100000.data-00001-of-00004&amp;quot; \
  &amp;quot;gs://t5-data/pretrained_models/t5.1.1.lm100k.large/model.ckpt-1100000.data-00002-of-00004&amp;quot; \
  &amp;quot;gs://t5-data/pretrained_models/t5.1.1.lm100k.large/model.ckpt-1100000.data-00003-of-00004&amp;quot; \
  &amp;quot;gs://t5-data/pretrained_models/t5.1.1.lm100k.large/model.ckpt-1100000.index&amp;quot; \
  &amp;quot;gs://t5-data/pretrained_models/t5.1.1.lm100k.large/model.ckpt-1100000.meta&amp;quot; \
  &amp;quot;gs://t5-data/pretrained_models/t5.1.1.lm100k.large/operative_config.gin&amp;quot; \
  .
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;to download LM-adapted T5-large checkpoint &lt;a href=&#34;https://console.cloud.google.com/storage/browser/t5-data/pretrained_models/t5.1.1.lm100k.large;tab=objects?prefix=&amp;amp;forceOnObjectsSortingFiltering=false&#34; target=&#34;_blank&#34;&gt;T5-large TF ckpt&lt;/a&gt;. Finally,&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd ..
mkdir lm_adapted_t5model/torch_ckpt
python convertmodel.py
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;3-run-code&#34;&gt;3. Run code&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Remember to change &lt;code&gt;lm_adapted_path&lt;/code&gt; and &lt;code&gt;cache_path&lt;/code&gt; in script files to the right path on your server!!!&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;lm_adapted_path: the path of lm_adapted t5 model (Pytorch ckpt)
cache_path: the path of huggingface cache
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;3-1-ner&#34;&gt;3.1. NER:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;cd NER
bash runall_1.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;3-2-classification&#34;&gt;3.2. Classification&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;cd Classification
bash Classification.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;3-3-summarization&#34;&gt;3.3. Summarization&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;cd Summarization
bash Summarization.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;3-4-different-task-types&#34;&gt;3.4. Different task types&lt;/h4&gt;

&lt;p&gt;Without forward knowledge transfer:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd DiffType/T5NoContinual
bash T5NoContinual.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;With forward knowledge transfer:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd DiffType/T5WithContinual
bash T5WithContinual.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;If you find our paper or this project helps your research, please kindly consider citing our paper in your publication.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;@inproceedings{
qin2022lfpt,
title={{LFPT}5: A Unified Framework for Lifelong Few-shot Language Learning Based on Prompt Tuning of T5},
author={Chengwei Qin and Shafiq Joty},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=HCRVf71PMF}
}

&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Rethinking Self-Supervision Objectives for Generalizable Coherence Modeling</title>
      <link>https://ntunlpsg.github.io/project/coherence-paradigm/</link>
      <pubDate>Thu, 10 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>https://ntunlpsg.github.io/project/coherence-paradigm/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;https://github.com/ntunlp/coherence-paradigm#readme&#34; target=&#34;_blank&#34;&gt;github&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Code and data for the ACL 2022 paper: &lt;a href=&#34;https://arxiv.org/abs/2110.07198&#34; target=&#34;_blank&#34;&gt;https://arxiv.org/abs/2110.07198&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;pre-requisites&#34;&gt;Pre-requisites&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;* Python=3.6
* Pytorch&amp;gt;=1.10.1
* Huggingface Transformers=4.13
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;input-data-format&#34;&gt;Input Data Format&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;All models require train, dev and test files in pickle format as input. The specific format is:

&lt;ul&gt;
&lt;li&gt;The pickle file should be a list of dictionaries. Each dictionary has two keys, &amp;lsquo;pos&amp;rsquo; and &amp;lsquo;negs&amp;rsquo; (or &amp;lsquo;neg&amp;rsquo; for pairwise data). &amp;lsquo;pos&amp;rsquo; should contain the` list of sentences from the positive or coherent document, while &amp;lsquo;negs&amp;rsquo; should contain the list of negative documents (e.g. incoherent documents, permutations) which are in turn lists of sentences.&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;--data_type&lt;/code&gt; argument should be set to &lt;code&gt;single&lt;/code&gt; or &lt;code&gt;multiple&lt;/code&gt; depending on the number of negatives in the dataset.

&lt;ul&gt;
&lt;li&gt;e.g. &lt;code&gt;single&lt;/code&gt;: [{&amp;lsquo;pos&amp;rsquo;:[&amp;lsquo;aa&amp;rsquo;, &amp;lsquo;bb&amp;rsquo;, &amp;lsquo;cc&amp;rsquo;], &amp;lsquo;neg&amp;rsquo;:[&amp;lsquo;bb&amp;rsquo;, &amp;lsquo;aa&amp;rsquo;, &amp;lsquo;cc&amp;rsquo;]}..{}]&lt;/li&gt;
&lt;li&gt;e.g. &lt;code&gt;multiple&lt;/code&gt;: [{&amp;lsquo;pos&amp;rsquo;:[&amp;lsquo;aa&amp;rsquo;, &amp;lsquo;bb&amp;rsquo;, &amp;lsquo;cc&amp;rsquo;], &amp;lsquo;negs&amp;rsquo;:[[&amp;lsquo;bb&amp;rsquo;, &amp;lsquo;aa&amp;rsquo;, &amp;lsquo;cc&amp;rsquo;], [&amp;lsquo;cc&amp;rsquo;, &amp;lsquo;aa&amp;rsquo;, &amp;lsquo;bb&amp;rsquo;], [&amp;lsquo;bb&amp;rsquo;, &amp;lsquo;cc&amp;rsquo;, &amp;lsquo;aa&amp;rsquo;]]}..{}]&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;training-the-model&#34;&gt;Training the model&lt;/h3&gt;

&lt;p&gt;Navigate into the model folder that you want to train (pairwise, contrastive or our full hard negative model with momentum encoder).&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; CUDA_VISIBLE_DEVICES=x python train.py --train_file [train.pkl] --dev_file [dev.pkl]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Please refer to the &lt;code&gt;args.py&lt;/code&gt; file for all other arguments that can be set. All hyperparameter defaults are set to the values used for experiments in the paper.&lt;/p&gt;

&lt;p&gt;To evaluate the model on a test set, run&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; CUDA_VISIBLE_DEVICES=x python eval.py --test_file [test.pkl] --data_type [single,multiple] --pretrained_model [saved_checkpoint.pt]
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;evaluation-using-the-coherence-model&#34;&gt;Evaluation using the Coherence Model&lt;/h3&gt;

&lt;p&gt;You can use our trained model to evaluate machine generated text. More details will be updated soon.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Representation learning with few human annotations</title>
      <link>https://ntunlpsg.github.io/talk/junnan/</link>
      <pubDate>Sun, 03 Oct 2021 00:00:00 +0800</pubDate>
      
      <guid>https://ntunlpsg.github.io/talk/junnan/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Speaker:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&#34;img-circle&#34; style=&#34;width: 180px;&#34; src=&#34;../../person/junnan.jpeg&#34;&gt; &lt;br&gt; &lt;center&gt; Junnan Li &lt;br&gt; Senior Research Scientist, Salesforce Research Asia, Singapore &lt;br&gt; &lt;a href=&#34;https://sites.google.com/site/junnanlics&#34; target=&#34;_blank&#34;&gt;Website&lt;/a&gt; |  &lt;a href=&#34;https://scholar.google.com/citations?user=MuUhwi0AAAAJ&amp;amp;hl=en&amp;amp;oi=ao&#34; target=&#34;_blank&#34;&gt;Google Scholar&lt;/a&gt; &lt;br&gt; Research topic: Self-supervised Learning, Semi-supervised Learning, Weakly-supervised Learning, Transfer Learning, Vision and Language &lt;br&gt; &lt;a href=&#34;https://teams.microsoft.com/registration/SJPOFSq-K0aPwOF2WpsgSg,mVAApF_uvE2vORk0OJju0w,EfHG1if7WEOly7k9bs3WYQ,0yaa_CtqpkmQRFdwoSF_TA,qy_q0roFTkGX10G2Zo7mPg,sUdmKIvuBkqsqwmxDcrTJQ?mode=read&amp;amp;tenantId=15ce9348-be2a-462b-8fc0-e1765a9b204a&#34; target=&#34;_blank&#34;&gt;Registration Link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Bio:&lt;/strong&gt; Junnan Li is currently a senior research scientist at Salesforce Research Asia. He obtained his PhD at the National University of Singapore in 2019. He has published in many top-tier venues in machine learning and computer vision, such as NeurIPS, ICLR, CVPR, ICCV, etc. His main research interests include self-supervised learning, semi-supervised learning, weakly-supervised learning, and vision-language learning. His ultimate research goal is to build general-purpose models that can self-learn without human involvement.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Unified Speaker Adaptation Approach for ASR</title>
      <link>https://ntunlpsg.github.io/project/asr/</link>
      <pubDate>Thu, 09 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>https://ntunlpsg.github.io/project/asr/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://github.com/zyzpower/gradprune_speaker&#34; target=&#34;_blank&#34;&gt;Follow the github repo&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;You will find the source code of the paper.&lt;/p&gt;

&lt;p&gt;This is the source code of our method proposed in paper &amp;ldquo;A Unified Speaker Adaptation Approach for ASR&amp;rdquo; accepted by EMNLP 2021.&lt;/p&gt;

&lt;p&gt;We use the transformer model in Espnet toolkit as the model architecture, and the gradual pruning is achieved by editing code in espnet/espnet/asr/pytorch_backend/asr.py.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>DVD: A Diagnostic Dataset for Multi-step Reasoning in Video Grounded Dialogue</title>
      <link>https://ntunlpsg.github.io/talk/henry/</link>
      <pubDate>Mon, 06 Sep 2021 00:00:00 +0800</pubDate>
      
      <guid>https://ntunlpsg.github.io/talk/henry/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Speaker:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&#34;img-circle&#34; style=&#34;width: 180px;&#34; src=&#34;../../person/henry.jpeg&#34;&gt; &lt;br&gt; &lt;center&gt; Henry Hung Le &lt;br&gt; Ph.D. Candidate, SMU &lt;br&gt; &lt;a href=&#34;https://sites.google.com/view/henryle2018/home&#34; target=&#34;_blank&#34;&gt;Website&lt;/a&gt; |  &lt;a href=&#34;https://scholar.google.com/citations?user=jnYI1UgAAAAJ&amp;amp;hl=en&#34; target=&#34;_blank&#34;&gt;Google Scholar&lt;/a&gt; &lt;br&gt; Research topic: Conversational AI, Video Understanding, Task-oriented Dialogues &lt;br&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Bio:&lt;/strong&gt; Hung Le is a Ph.D. candidate in Computer Science at Singapore Management University, advised by Prof. Steven Hoi and Dr. Nancy Chen. His research interest lies in natural language processing and machine learning, including research in non-autoregressive dialogue state tracking, task-oriented dialogues, and video-grounded dialogues. His research work has been published in ACL, ICLR, EMNLP, and AAAI. Hung is awarded the Computer and Information Science Scholarship by A*STAR Singapore and is a recipient of the Presidential Doctoral Fellowship from 2019 to 2021.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Towards Generating Human-like Deep Questions</title>
      <link>https://ntunlpsg.github.io/talk/liangming/</link>
      <pubDate>Mon, 09 Aug 2021 00:00:00 +0800</pubDate>
      
      <guid>https://ntunlpsg.github.io/talk/liangming/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Speaker:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&#34;img-circle&#34; style=&#34;width: 180px;&#34; src=&#34;../../person/liangming.jpeg&#34;&gt; &lt;br&gt; &lt;center&gt; Liangming Pan &lt;br&gt; Ph.D. Candidate, NUS. &lt;br&gt; &lt;a href=&#34;http://www.liangmingpan.com/&#34; target=&#34;_blank&#34;&gt;Website&lt;/a&gt; |  &lt;a href=&#34;https://scholar.google.com/citations?user=JcjjOTUAAAAJ&amp;amp;hl=en&amp;amp;oi=ao&#34; target=&#34;_blank&#34;&gt;Google Scholar&lt;/a&gt; &lt;br&gt; Research topic: Text Generation, Knowledge Graph, Multi-media Learning &lt;br&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Bio:&lt;/strong&gt; Liangming Pan is a fourth year Computer Science Ph.D. student at National University of Singapore, jointly advised by Prof. Min-Yen Kan and Prof. Tat-Seng Chua. He was also a visiting Ph.D. student at UC Santa Barbara in 2020. Prior to joining NUS, he received a Master degree from School of Computer Science at Tsinghua University. His board research interests include knowledge base, natural language processing, and data mining. His Ph.D. research topics focus on natural language generation with deep reasoning, including neural question generation, and text style transfer. He has published several research papers in top-ranked conferences including ACL, COLING, and NAACL. He has also won the Research Achievement Award and the Dean&amp;rsquo;s Graduate Award from NUS School of Computing.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>MulDA: A Multilingual Data Augmentation Framework for Low-Resource Cross-Lingual NER</title>
      <link>https://ntunlpsg.github.io/project/mulda/</link>
      <pubDate>Wed, 04 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://ntunlpsg.github.io/project/mulda/</guid>
      <description>

&lt;h1 id=&#34;mulda&#34;&gt;MulDA&lt;/h1&gt;

&lt;p&gt;This &lt;a href=&#34;https://github.com/ntunlp/mulda.git&#34; target=&#34;_blank&#34;&gt;repository&lt;/a&gt; contains the source code and data used in our paper &amp;ldquo;MulDA: A Multilingual Data Augmentation Framework for Low-Resource Cross-Lingual NER&amp;rdquo; accepted by ACL-IJCNLP-2021.&lt;/p&gt;

&lt;h2 id=&#34;data&#34;&gt;Data&lt;/h2&gt;

&lt;p&gt;The data generated using our labeled sequence translation method can be found in the &amp;ldquo;data&amp;rdquo; directory.&lt;/p&gt;

&lt;h2 id=&#34;labled-sequence-translation&#34;&gt;Labled Sequence Translation&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;cd code/translate; python translate.py
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;lstm-lm-multiilngual-lstm-language-model&#34;&gt;lstm-lm: multiilngual LSTM language model&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;train lstm-lm on linearized sequences&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd code/lstm-lm;

python train.py \
--train_file PATH/TO/train.linearized.txt \
--valid_file PATH/TO/dev.linearized.txt \
--model_file PATH/TO/model.pt \
--emb_dim 300 \
--rnn_size 512 \
--gpuid 0 
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;generate linearized sequences&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd code/lstm-lm;

python generate.py \
--model_file PATH/TO/model.pt \
--out_file PATH/TO/out.txt \
--num_sentences 10000 \
--temperature 1.0 \
--seed 3435 \
--max_sent_length 32 \
--gpuid 0
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;tools-tools-for-data-processing&#34;&gt;tools: tools for data processing&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;tools/preprocess.py: sequence linearization&lt;/li&gt;
&lt;li&gt;tools/line2cols.py: convert linearized sequence back to two-column format&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;requirements&#34;&gt;Requirements&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;code/lstm-lm/requirements.txt&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;citation&#34;&gt;Citation&lt;/h1&gt;

&lt;p&gt;Please cite our paper if you found the resources in this repository useful.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;@inproceedings{liu-etal-2021-mulda,
    title = &amp;quot;MulDA: A Multilingual Data Augmentation Framework for Low-Resource Cross-Lingual NER&amp;quot;,
    author = &amp;quot;Liu Linlin  and
      Ding, Bosheng  and
      Bing, Lidong  and
      Joty, Shafiq  and
      Si, Luo  and
      Miao, Chunyan&amp;quot;,
    booktitle = &amp;quot;Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics (ACL&#39;21)&amp;quot;,
    month = aug,
    year = &amp;quot;2021&amp;quot;,
    address = &amp;quot;Online&amp;quot;,
    publisher = &amp;quot;Association for Computational Linguistics&amp;quot;,
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Structured Pointing Networks for Natural Language Understanding</title>
      <link>https://ntunlpsg.github.io/talk/thomas/</link>
      <pubDate>Sun, 04 Jul 2021 00:00:00 +0800</pubDate>
      
      <guid>https://ntunlpsg.github.io/talk/thomas/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Speaker:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&#34;img-circle&#34; style=&#34;width: 180px;&#34; src=&#34;../../person/thomas.jpg&#34;&gt; &lt;br&gt; &lt;center&gt; Thanh-Tung Nguyen &lt;br&gt; Ph.D. Candidate, NTU. &lt;br&gt; &lt;a href=&#34;https://www.linkedin.com/in/tungngthanh/&#34; target=&#34;_blank&#34;&gt;Linkedin&lt;/a&gt; |  &lt;a href=&#34;https://scholar.google.com/citations?user=NkKC6zYAAAAJ&amp;amp;hl=en&#34; target=&#34;_blank&#34;&gt;Google Scholar&lt;/a&gt; | &lt;a href=&#34;https://tungngthanh.github.io/&#34; target=&#34;_blank&#34;&gt;Website&lt;/a&gt; &lt;br&gt; Research topic: Parsing, RNN, Machine Translation &lt;br&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Bio:&lt;/strong&gt; Thomas is a Ph.D candidate in &lt;a href=&#34;https://ntunlpsg.github.io/&#34; target=&#34;_blank&#34;&gt;NLP Research Group&lt;/a&gt; at the &lt;a href=&#34;https://www.ntu.edu.sg/&#34; target=&#34;_blank&#34;&gt;Nanyang Technological University (NTU)&lt;/a&gt;. He is also affiliated with the Machine Intellection Department (previously was Data Analytics &amp;amp; Deep Learning 2.0 Department) of Institution for Infocomm Research (I2R). His research involves various aspects of NLP, especially NLP tools and NLP applications. Thomas will join I2R to work on the Collaborative AI project after his graduation.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>NTU-NLP: Monthly NLP Talks</title>
      <link>https://ntunlpsg.github.io/post/monthly-nlp-talk/</link>
      <pubDate>Tue, 29 Jun 2021 00:00:00 +0800</pubDate>
      
      <guid>https://ntunlpsg.github.io/post/monthly-nlp-talk/</guid>
      <description>

&lt;p&gt;The NTU-NLP group will be hosting a series of talks, featuring local NLP researchers and scientists. The seminars will be organized monthly. The goal is to create a platform for local NLP/ML enthusiasts to share their work and to encourage collaborations across local universities and organizations.&lt;/p&gt;

&lt;p&gt;Ph.D&amp;rsquo;s, Postdocs, and company scientists are all welcome to participate. &lt;strong&gt;Interested Speakers&lt;/strong&gt; please send their inquiry to &lt;code&gt;ruochen002@e.ntu.edu.sg&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&#34;subscription&#34;&gt;Subscription&lt;/h2&gt;

&lt;p&gt;To subscribe to a mailing list of upcoming talks, please follow either of the steps:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Please go to &lt;a href=&#34;http://groups.google.com&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Google Groups&lt;/strong&gt;&lt;/a&gt; and search &amp;ldquo;ntu-nlp-sg-seminar&amp;rdquo; under &lt;strong&gt;All groups and messages&lt;/strong&gt; tab.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Email with subject &lt;strong&gt;ntu-nlp-sg-seminar&lt;/strong&gt; to &lt;code&gt;ntunlpsg@gmail.com&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;schedule&#34;&gt;Schedule&lt;/h2&gt;

&lt;!-- ### Time: &lt;span style=&#34;color:red&#34;&gt;TBA&lt;/span&gt; --&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;&lt;center&gt; Date &lt;/center&gt;&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;&lt;center&gt;Description &lt;/center&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;br&gt; &lt;br&gt; &lt;br&gt; &lt;br&gt; &lt;br&gt; &lt;p align=&#34;center&#34;&gt; 11 October, 2021 &lt;br&gt; 4:30 - 5:30 PM &lt;br&gt; (UTC +8) &lt;/p&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;img class=&#34;img-circle&#34; style=&#34;width: 180px;&#34; src=&#34;../../person/junnan.jpeg&#34;&gt; &lt;br&gt; Junnan Li &lt;br&gt; Senior Research Scientist, Salesforce Research Asia, Singapore &lt;br&gt; &lt;a href=&#34;https://sites.google.com/site/junnanlics&#34; target=&#34;_blank&#34;&gt;Website&lt;/a&gt; |  &lt;a href=&#34;https://scholar.google.com/citations?user=MuUhwi0AAAAJ&amp;amp;hl=en&amp;amp;oi=ao&#34; target=&#34;_blank&#34;&gt;Google Scholar&lt;/a&gt; &lt;br&gt; Research topic:  Self-supervised Learning, Semi-supervised Learning, Weakly-supervised Learning, Transfer Learning, Vision and Language &lt;br&gt; &lt;a href=&#34;../../talk/junnan&#34;&gt;Talk Details&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://teams.microsoft.com/registration/SJPOFSq-K0aPwOF2WpsgSg,mVAApF_uvE2vORk0OJju0w,EfHG1if7WEOly7k9bs3WYQ,0yaa_CtqpkmQRFdwoSF_TA,qy_q0roFTkGX10G2Zo7mPg,sUdmKIvuBkqsqwmxDcrTJQ?mode=read&amp;amp;tenantId=15ce9348-be2a-462b-8fc0-e1765a9b204a&#34; target=&#34;_blank&#34;&gt;Registration Link&lt;/a&gt; &lt;br&gt; &lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/t3EyTMvFCAE&#34; frameborder=&#34;0&#34; allow=&#34;autoplay; encrypted-media&#34; allowfullscreen&gt;&lt;/iframe&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;br&gt; &lt;br&gt; &lt;br&gt; &lt;br&gt; &lt;br&gt; &lt;p align=&#34;center&#34;&gt; 13 September, 2021 &lt;br&gt; 4:30 - 5:30 PM &lt;br&gt; (UTC +8) &lt;/p&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;img class=&#34;img-circle&#34; style=&#34;width: 180px;&#34; src=&#34;../../person/henry.jpeg&#34;&gt; &lt;br&gt; Henry Hung Le &lt;br&gt; Ph.D. Candidate, SMU. &lt;br&gt; &lt;a href=&#34;https://sites.google.com/view/henryle2018/home&#34; target=&#34;_blank&#34;&gt;Website&lt;/a&gt; |  &lt;a href=&#34;https://scholar.google.com/citations?user=jnYI1UgAAAAJ&amp;amp;hl=en&#34; target=&#34;_blank&#34;&gt;Google Scholar&lt;/a&gt; &lt;br&gt; Research topic: Conversational AI, Video Understanding, Task-oriented Dialogues &lt;br&gt; &lt;a href=&#34;../../talk/henry&#34;&gt;Talk Details&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://teams.microsoft.com/registration/SJPOFSq-K0aPwOF2WpsgSg,mVAApF_uvE2vORk0OJju0w,EfHG1if7WEOly7k9bs3WYQ,85H58boEIUi55Ji9ruGeIg,Qe-QTrzJSE-B569L2e8Rhg,8SgWAxByEUie0twEcIce3g?mode=read&amp;amp;tenantId=15ce9348-be2a-462b-8fc0-e1765a9b204a&#34; target=&#34;_blank&#34;&gt;Registration Link&lt;/a&gt;&lt;br&gt; &lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/rjEFtYNmoMs&#34; frameborder=&#34;0&#34; allow=&#34;autoplay; encrypted-media&#34; allowfullscreen&gt;&lt;/iframe&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;br&gt; &lt;br&gt; &lt;br&gt; &lt;br&gt; &lt;br&gt; &lt;p align=&#34;center&#34;&gt; 16 August, 2021 &lt;br&gt; 2:00 - 3:00 PM &lt;br&gt; (UTC +8) &lt;/p&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;img class=&#34;img-circle&#34; style=&#34;width: 180px;&#34; src=&#34;../../person/liangming.jpeg&#34;&gt; &lt;br&gt; Liangming Pan &lt;br&gt; Ph.D. Candidate, NUS. &lt;br&gt; &lt;a href=&#34;http://www.liangmingpan.com/&#34; target=&#34;_blank&#34;&gt;Website&lt;/a&gt; |  &lt;a href=&#34;https://scholar.google.com/citations?user=JcjjOTUAAAAJ&amp;amp;hl=en&amp;amp;oi=ao&#34; target=&#34;_blank&#34;&gt;Google Scholar&lt;/a&gt; &lt;br&gt; Research topic: Text Generation, Knowledge Graph, Multi-media Learning &lt;br&gt; &lt;a href=&#34;../../talk/liangming&#34;&gt;Talk Details&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://teams.microsoft.com/registration/SJPOFSq-K0aPwOF2WpsgSg,mVAApF_uvE2vORk0OJju0w,EfHG1if7WEOly7k9bs3WYQ,v1_XYvcKHUiL5HYlH8upQQ,dcx2ACSf_k-3a8mWBEdqgw,gAgX1m0VUEO6SVNHt7wv1g?mode=read&amp;amp;tenantId=15ce9348-be2a-462b-8fc0-e1765a9b204a&#34; target=&#34;_blank&#34;&gt;Registration Link&lt;/a&gt;&lt;br&gt; &lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/CAJJJOoS-q4&#34; frameborder=&#34;0&#34; allow=&#34;autoplay; encrypted-media&#34; allowfullscreen&gt;&lt;/iframe&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;br&gt; &lt;br&gt; &lt;br&gt; &lt;br&gt; &lt;br&gt; &lt;p align=&#34;center&#34;&gt; 12 July, 2021 &lt;br&gt; 4:30 - 5:30 PM &lt;br&gt; (UTC +8) &lt;/p&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;img class=&#34;img-circle&#34; style=&#34;width: 180px;&#34; src=&#34;../../person/thomas.jpg&#34;&gt; &lt;br&gt; Thanh-Tung Nguyen (Thomas) &lt;br&gt; Ph.D. Candidate, NTU. &lt;br&gt; &lt;a href=&#34;https://www.linkedin.com/in/tungngthanh/&#34; target=&#34;_blank&#34;&gt;Linkedin&lt;/a&gt; |  &lt;a href=&#34;https://scholar.google.com/citations?user=NkKC6zYAAAAJ&amp;amp;hl=en&#34; target=&#34;_blank&#34;&gt;Google Scholar&lt;/a&gt; | &lt;a href=&#34;https://tungngthanh.github.io/&#34; target=&#34;_blank&#34;&gt;Website&lt;/a&gt; &lt;br&gt; Research topic: Parsing, RNN, Machine Translation &lt;br&gt; &lt;a href=&#34;../../talk/thomas&#34;&gt;Talk Details&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://teams.microsoft.com/registration/SJPOFSq-K0aPwOF2WpsgSg,mVAApF_uvE2vORk0OJju0w,EfHG1if7WEOly7k9bs3WYQ,FCCJZJ3vzEiY9OmmZMJ9iQ,H0hzNw4NykeFJTivdszM2w,8pP5g8ZiLEaenaJ2y9X3RA?mode=read&amp;amp;tenantId=15ce9348-be2a-462b-8fc0-e1765a9b204a&#34; target=&#34;_blank&#34;&gt;Registration Link&lt;/a&gt; &lt;br&gt; &lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/HTKIM9_D71I&#34; frameborder=&#34;0&#34; allow=&#34;autoplay; encrypted-media&#34; allowfullscreen&gt;&lt;/iframe&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&#34;organizing-team&#34;&gt;Organizing Team&lt;/h2&gt;

&lt;!-- | | | |
| :---: | :---: | :---: |
|&lt;img class=&#34;img-circle&#34; style=&#34;width: 80px;&#34; src=&#34;../../person/shafiq.jpg&#34;&gt; | &lt;img class=&#34;img-circle&#34; style=&#34;width: 80px;&#34; src=&#34;../../person/ruochen.jpg&#34;&gt; | &lt;img class=&#34;img-circle&#34; style=&#34;width: 80px;&#34; src=&#34;../../person/ravaut.jpg&#34;&gt; |
| [Shafiq Joty](https://raihanjoty.github.io/) | [Ruochen Zhao](https://www.linkedin.com/in/esther-ruochen-zhao-855357150/) | [Mathieu RAVAUT](https://www.linkedin.com/in/mravox/) |
| &lt;img class=&#34;img-circle&#34; style=&#34;width: 80px;&#34; src=&#34;../../person/Chen.jpg&#34;&gt; | &lt;img class=&#34;img-circle&#34; style=&#34;width: 80px;&#34; src=&#34;../../person/saiful.jpg&#34;&gt; | &lt;img class=&#34;img-circle&#34; style=&#34;width: 80px;&#34; src=&#34;../../person/LinXiang.jpg&#34;&gt; |
| [Hailin Chen](https://www.linkedin.com/in/chenhailin/) | [M Saiful Bari](https://sbmaruf.github.io) | [Lin Xiang](https://shawnlimn.github.io) | --&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;img class=&#34;img-circle&#34; style=&#34;width: 80px;&#34; src=&#34;../../person/shafiq.jpg&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;img class=&#34;img-circle&#34; style=&#34;width: 80px;&#34; src=&#34;../../person/ruochen.jpg&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;img class=&#34;img-circle&#34; style=&#34;width: 80px;&#34; src=&#34;../../person/ravaut.jpg&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;img class=&#34;img-circle&#34; style=&#34;width: 80px;&#34; src=&#34;../../person/Chen.jpg&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;img class=&#34;img-circle&#34; style=&#34;width: 80px;&#34; src=&#34;../../person/saiful.jpg&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;img class=&#34;img-circle&#34; style=&#34;width: 80px;&#34; src=&#34;../../person/LinXiang.jpg&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;small&gt; &lt;a href=&#34;https://raihanjoty.github.io/&#34; target=&#34;_blank&#34;&gt;Shafiq Joty&lt;/a&gt; &lt;/small&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;small&gt; &lt;a href=&#34;https://www.linkedin.com/in/esther-ruochen-zhao-855357150/&#34; target=&#34;_blank&#34;&gt;Ruochen Zhao&lt;/a&gt; &lt;/small&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;small&gt;&lt;a href=&#34;https://www.linkedin.com/in/mravox/&#34; target=&#34;_blank&#34;&gt;Mathieu RAVAUT&lt;/a&gt;&lt;/small&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;small&gt;&lt;a href=&#34;https://www.linkedin.com/in/chenhailin/&#34; target=&#34;_blank&#34;&gt;Hailin Chen&lt;/a&gt;&lt;/small&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;small&gt;&lt;a href=&#34;https://sbmaruf.github.io&#34; target=&#34;_blank&#34;&gt;M Saiful Bari&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;small&gt;&lt;a href=&#34;https://shawnlimn.github.io&#34; target=&#34;_blank&#34;&gt;Lin Xiang&lt;/a&gt; &lt;/small&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</description>
    </item>
    
    <item>
      <title>Straight to the Gradient: Learning to Use Novel Tokens for Neural Text Generation</title>
      <link>https://ntunlpsg.github.io/publication/scalegrad/</link>
      <pubDate>Thu, 20 May 2021 00:00:00 +0000</pubDate>
      
      <guid>https://ntunlpsg.github.io/publication/scalegrad/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
