<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>NTU-NLP on NTU-NLP</title>
    <link>https://ntunlpsg.github.io/</link>
    <description>Recent content in NTU-NLP on NTU-NLP</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>ntunlp &amp;copy; 2020</copyright>
    <lastBuildDate>Thu, 20 May 2021 00:00:00 +0000</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Recent Posts</title>
      <link>https://ntunlpsg.github.io/post/post/</link>
      <pubDate>Mon, 10 Sep 2018 00:00:00 +0800</pubDate>
      
      <guid>https://ntunlpsg.github.io/post/post/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Resources</title>
      <link>https://ntunlpsg.github.io/resources/list/</link>
      <pubDate>Wed, 20 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>https://ntunlpsg.github.io/resources/list/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Straight to the Gradient: Learning to Use Novel Tokens for Neural Text Generation</title>
      <link>https://ntunlpsg.github.io/publication/scalegrad/</link>
      <pubDate>Thu, 20 May 2021 00:00:00 +0000</pubDate>
      
      <guid>https://ntunlpsg.github.io/publication/scalegrad/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Cross-model Back-translated Distillation for Unsupervised Machine Translation</title>
      <link>https://ntunlpsg.github.io/publication/cbd_icml_2021/</link>
      <pubDate>Sat, 15 May 2021 00:00:00 +0000</pubDate>
      
      <guid>https://ntunlpsg.github.io/publication/cbd_icml_2021/</guid>
      <description></description>
    </item>
    
    <item>
      <title>AugVic: Exploiting BiText Vicinity for Low-Resource NMT</title>
      <link>https://ntunlpsg.github.io/publication/augvic/</link>
      <pubDate>Sat, 01 May 2021 00:00:00 +0000</pubDate>
      
      <guid>https://ntunlpsg.github.io/publication/augvic/</guid>
      <description></description>
    </item>
    
    <item>
      <title>RST Parsing from Scratch</title>
      <link>https://ntunlpsg.github.io/project/naacl21-rst-parsing-resource/naacl21-rst-parsing-resource/</link>
      <pubDate>Sat, 01 May 2021 00:00:00 +0000</pubDate>
      
      <guid>https://ntunlpsg.github.io/project/naacl21-rst-parsing-resource/naacl21-rst-parsing-resource/</guid>
      <description>

&lt;h1 id=&#34;rst-parsing-from-scratch&#34;&gt;RST Parsing from Scratch&lt;/h1&gt;

&lt;p&gt;This repository contains the source code of our paper &lt;a href=&#34;https://arxiv.org/abs/2105.10861&#34; target=&#34;_blank&#34;&gt;RST Parsing from Scratch&lt;/a&gt; in NAACL 2021.&lt;/p&gt;

&lt;h2 id=&#34;requirements&#34;&gt;Requirements&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;python&lt;/code&gt;: 3.7&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pytorch&lt;/code&gt;: 1.4&lt;/li&gt;
&lt;li&gt;&lt;code&gt;transformers&lt;/code&gt;: 3.0&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;usage&#34;&gt;Usage&lt;/h2&gt;

&lt;p&gt;To train a discourse parser:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./*_train.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To predict discourse tree:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./*_predict.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;data-format&#34;&gt;Data Format&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;For end-to-end parsing from scratch (no sentence guidance):
we need to create the data with dummy edu_break and doc_structure. Refer to &lt;code&gt;create_sample_dummy_format_data.py&lt;/code&gt; and &lt;code&gt;dummy_format_data/sample_rawtext_data_format&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;For other parsing models:
Refer to &lt;code&gt;create_sample_dummy_format_data.py&lt;/code&gt; and &lt;code&gt;dummy_format_data/sample_full_data_format&lt;/code&gt;
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;Please cite our paper if you found the resources in this repository useful.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;@inproceedings{nguyen-etal-2021-rst-scratch,
title = &amp;quot;RST Parsing from Scratch&amp;quot;,
author = &amp;quot;Nguyen, Thanh-Tung  and
  Nguyen, Xuan-Phi  and
  Joty, Shafiq  and
  Li, Xiaoli&amp;quot;,
booktitle = &amp;quot;Proceedings of the 2021 Conference of the North {A}merican Chapter 
of the Association for Computational Linguistics: Human Language Technologies, 
Volume 1 (Long and Short Papers)&amp;quot;,
month = jun,
year = &amp;quot;2021&amp;quot;,
address = &amp;quot;Online&amp;quot;,
publisher = &amp;quot;Association for Computational Linguistics&amp;quot;,
url = &amp;quot;https://arxiv.org/abs/2105.10861&amp;quot;,
doi = &amp;quot;&amp;quot;,
pages = &amp;quot;xx--xx&amp;quot;,}
}   
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>UXLA: A Robust Unsupervised Data Augmentation Framework for Cross-Lingual NLP</title>
      <link>https://ntunlpsg.github.io/publication/uxla/</link>
      <pubDate>Sat, 01 May 2021 00:00:00 +0000</pubDate>
      
      <guid>https://ntunlpsg.github.io/publication/uxla/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Rethinking Coherence Modeling: Synthetic vs. Downstream Tasks</title>
      <link>https://ntunlpsg.github.io/project/coherence/coh-eval/</link>
      <pubDate>Fri, 16 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://ntunlpsg.github.io/project/coherence/coh-eval/</guid>
      <description>

&lt;h3 id=&#34;about&#34;&gt;About&lt;/h3&gt;

&lt;p&gt;This resource contains the information regarding code and data used for evaluation in &amp;ldquo;&lt;a href=&#34;https://arxiv.org/pdf/2004.14626.pdf&#34; target=&#34;_blank&#34;&gt;Rethinking Coherence Modeling: Synthetic vs. Downstream Tasks&lt;/a&gt;&amp;rdquo; paper (EACL 2021).
&lt;br&gt;&lt;/p&gt;

&lt;h3 id=&#34;source-code-of-evaluated-coherence-models&#34;&gt;Source code of evaluated coherence models&lt;/h3&gt;

&lt;p&gt;We benchmark the performance of five coherence models. For each of the coherence models, we conducted experiments with publicly available codes from the respective authors. Links are given below:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://bitbucket.org/melsner/browncoherence/&#34; target=&#34;_blank&#34;&gt;Entity Grid&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/datienguyen/cnn_coherence&#34; target=&#34;_blank&#34;&gt;Neural Entity Grid&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/taasnim/conv-coherence&#34; target=&#34;_blank&#34;&gt;Lexicalized Neural Entity Grid&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/BorealisAI/cross_domain_coherence&#34; target=&#34;_blank&#34;&gt;Transferable Neural Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/taasnim/unified-coherence-model&#34; target=&#34;_blank&#34;&gt;Unified Neural Model&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;datasets&#34;&gt;Datasets&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Machine Translation Evaluation Dataset: We use the reference and the system translations provided by WMT2017-2018 as our test data, under the assumption that the reference translations are more coherent than the system translations. This results in a testset of 20,680 reference-system translation document-pairs.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Abstractive Summarization Evaluation Dataset: We use the CNN/DM dataset for this task. We collect the reference summaries from the CNN/DM testset as well as the summaries generated by the four representative abstractive summarization systems: Pointer-Generator, BertSumExtAbs(BSEA), UniLM, and SENECA.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Extractive Summarization Evaluation Dataset: The dataset from  Barzilay  and  Lapata(2008) provides 16 sets of summaries where each set corresponds to a multi-document cluster and contains summaries generated by 5 systems and 1 human.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Next Utterance Ranking Evaluation Dataset: We evaluated the coherence models on both datasets of the &lt;a href=&#34;https://github.com/dstc8-track2/NOESIS-II/}{https://github.com/dstc8-track2/NOESIS-II/&#34; target=&#34;_blank&#34;&gt;DSTC8 response selection track&lt;/a&gt;, i.e., the Advising and Ubuntu datasets. The former contains two-party dialogs that simulate a discussion between a student and an academic advisor, while the latter consists of multi-party conversations extracted from the Ubuntu IRC channel.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The datasets can be found in this &lt;a href=&#34;https://drive.google.com/drive/folders/1qaGafR6eJZUtnpNT4CUj-Gdwlwlt9ZSV?usp=sharing&#34; target=&#34;_blank&#34;&gt;LINK&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;citation&#34;&gt;Citation&lt;/h3&gt;

&lt;p&gt;Please cite our paper if you found the resources in this repository useful.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
@inproceedings{mohiuddin-etal-2021-rethinking,
      title={Rethinking Coherence Modeling: Synthetic vs. Downstream Tasks}, 
      author={Tasnim Mohiuddin and Prathyusha Jwalapuram and Xiang Lin and Shafiq Joty},
      year={2021},,
    booktitle = &amp;quot;Proceedings of the 16th Annual Meeting of the European chapter of the Association for Computational Linguistics (EACL) 2021&amp;quot;,
}


&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;licence&#34;&gt;Licence&lt;/h1&gt;

&lt;p&gt;MIT licence.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>DAGA: Data Augmentation with a Generation Approach for Low-resource Tagging Tasks</title>
      <link>https://ntunlpsg.github.io/project/daga/</link>
      <pubDate>Wed, 24 Feb 2021 00:00:00 +0800</pubDate>
      
      <guid>https://ntunlpsg.github.io/project/daga/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;https://github.com/ntunlp/daga&#34; target=&#34;_blank&#34;&gt;Github&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;daga&#34;&gt;DAGA&lt;/h1&gt;

&lt;p&gt;This is the source code of our method proposed in paper &amp;ldquo;&lt;a href=&#34;https://www.aclweb.org/anthology/2020.emnlp-main.488/&#34; target=&#34;_blank&#34;&gt;DAGA: Data Augmentation with a Generation Approach for Low-resource Tagging Tasks&lt;/a&gt;&amp;rdquo; accepted by EMNLP 2020.&lt;/p&gt;

&lt;h1 id=&#34;examples&#34;&gt;Examples&lt;/h1&gt;

&lt;h2 id=&#34;flair-seq-tagger-sequense-tagging-model&#34;&gt;flair_seq_tagger: sequense tagging model&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;cd flair_seq_tagger;

python train_tagger.py \
  --data_dir PATH/TO/TRAIN_DIR \
  --train_file  train.txt \
  --dev_file  dev.txt \
  --data_columns text ner \
  --model_dir ./model \
  --comment_symbol &amp;quot;__label__&amp;quot; \
  --embeddings_file PATH/TO/emb \
  --optim adam \
  --learning_rate 0.001 --min_learning_rate 0.00001 \
  --patience 2 \
  --max_epochs 100 \
  --hidden_size 512 \
  --mini_batch_size 32 \
  --gpuid 0
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;lstm-lm-lstm-language-model&#34;&gt;lstm-lm: LSTM language model&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;train lstm-lm on linearized sequences&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd lstm-lm;

python train.py \
--train_file PATH/TO/train.linearized.txt \
--valid_file PATH/TO/dev.linearized.txt \
--model_file PATH/TO/model.pt \
--emb_dim 300 \
--rnn_size 512 \
--gpuid 0 
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;generate linearized sequences&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd lstm-lm;

python generate.py \
--model_file PATH/TO/model.pt \
--out_file PATH/TO/out.txt \
--num_sentences 10000 \
--temperature 1.0 \
--seed 3435 \
--max_sent_length 32 \
--gpuid 0
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;tools-tools-for-data-processing&#34;&gt;tools: tools for data processing&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;preprocess.py: sequence linearization&lt;/li&gt;
&lt;li&gt;line2cols.py: convert linearized sequence back to two-column format&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;requirements&#34;&gt;Requirements&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;flair_seq_tagger/requirements.txt&lt;/li&gt;
&lt;li&gt;lstm-lm/requirements.txt&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;citation&#34;&gt;Citation&lt;/h1&gt;

&lt;p&gt;Please cite our paper if you found the resources in this repository useful.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;@inproceedings{ding-etal-2020-daga,
    title = &amp;quot;{DAGA}: Data Augmentation with a Generation Approach for Low-resource Tagging Tasks&amp;quot;,
    author = &amp;quot;Ding, Bosheng  and
      Liu, Linlin  and
      Bing, Lidong  and
      Kruengkrai, Canasai  and
      Nguyen, Thien Hai  and
      Joty, Shafiq  and
      Si, Luo  and
      Miao, Chunyan&amp;quot;,
    booktitle = &amp;quot;Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)&amp;quot;,
    month = nov,
    year = &amp;quot;2020&amp;quot;,
    address = &amp;quot;Online&amp;quot;,
    publisher = &amp;quot;Association for Computational Linguistics&amp;quot;,
    url = &amp;quot;https://www.aclweb.org/anthology/2020.emnlp-main.488&amp;quot;,
    doi = &amp;quot;10.18653/v1/2020.emnlp-main.488&amp;quot;,
    pages = &amp;quot;6045--6057&amp;quot;,
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Data Diversification: A Simple Strategy For Neural Machine Translation</title>
      <link>https://ntunlpsg.github.io/publication/data_diverse/</link>
      <pubDate>Tue, 01 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://ntunlpsg.github.io/publication/data_diverse/</guid>
      <description></description>
    </item>
    
    <item>
      <title>DAGA: Data Augmentation with a Generation Approach for Low-resource Tagging Tasks</title>
      <link>https://ntunlpsg.github.io/publication/daga/</link>
      <pubDate>Wed, 18 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>https://ntunlpsg.github.io/publication/daga/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Online Conversation Disentanglement with Pointer Networks</title>
      <link>https://ntunlpsg.github.io/publication/conversationdisentangle/</link>
      <pubDate>Mon, 16 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>https://ntunlpsg.github.io/publication/conversationdisentangle/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Pronoun-Targeted Fine-tuning for NMT with Hybrid Losses</title>
      <link>https://ntunlpsg.github.io/publication/pronoun-finetune/</link>
      <pubDate>Sun, 01 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>https://ntunlpsg.github.io/publication/pronoun-finetune/</guid>
      <description></description>
    </item>
    
    <item>
      <title>An Attention-based Rumor Detection Model with Tree-structured Recursive Neural Networks</title>
      <link>https://ntunlpsg.github.io/publication/majing_tist20/</link>
      <pubDate>Wed, 01 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://ntunlpsg.github.io/publication/majing_tist20/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Conversational agents in healthcare: a scoping review and conceptual analysis</title>
      <link>https://ntunlpsg.github.io/publication/cat_jmir20/</link>
      <pubDate>Wed, 01 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://ntunlpsg.github.io/publication/cat_jmir20/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
