+++
abstract = "Transfer learning has yielded state-of-the-art results in many supervised natural language processing tasks. However, annotated data for every target task in every target language is rare, especially for low-resource languages. In this work, we propose MultiMix, a novel data augmentation method for semi-supervised learning in zero-shot transfer learning scenarios. In particular, MultiMix targets to solve cross-lingual adaptation problems from a source (language) distribution to an unknown target (language) distribution assuming it has no training labels in the target language task. In its heart, MultiMix performs simultaneous self-training with data augmentation and unsupervised sample selection. To show its effectiveness, we have performed extensive experiments on zero-shot transfers for cross-lingual named entity recognition (XNER) and natural language inference (XNLI). Our experiments show sizeable improvements in both tasks outperforming the baselines by a good margin."
authors = ["M Saiful Bari", "Tasnim Mohiuddin", "Shafiq Joty"]
date = "2020-07-01"
image = ""
image_preview = ""
math = false
publication_types = ["1"]
selected = true
publication = "preprint"
title = "MultiMix: A Robust Data Augmentation Strategy for Cross-Lingual NLP"
url_code = ""
url_dataset = ""
url_pdf = "https://arxiv.org/pdf/2004.13240.pdf"
url_project = ""
url_slides = ""
url_video = ""
+++