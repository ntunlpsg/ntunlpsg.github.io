+++ abstract = "Submodularity is a desirable property for a variety of objectives in content selection where the current neural encoder-decoder framework is deficient. We propose diminishing attentions, a class of novel attention mechanisms that exploit the properties of submodular functions. The resulting attention module offers an architecturally simple yet empirically effective method to improve the coverage of neural text generation. We run on three directed text generation tasks with different levels of recovering rate, across two modalities, three neural model architectures and two training strategy variations. The results and analyses demonstrate that our method generalizes well across these settings, produces texts of good quality, outperforms comparable baselines and achieves state-of-the-art performance." 
authors = ["Simeng Han", "Xiang Lin", "Shafiq Joty"] 
date = "2020-06-13" 
image = "" 
image_preview = "" 
math = false 
publication_types = ["1"] 
selected = true 
publication = "preprint" 
title = "Resurrecting Submodularity for Neural Text Generation" 
url_code = "" 
url_dataset = "" 
url_pdf = "https://arxiv.org/pdf/1911.03014.pdf" 
url_project = "" 
url_slides = "" 
url_video = "" 
+++