<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>representation-learning on NTU-NLP</title>
    <link>https://ntunlpsg.github.io/tags/representation-learning/</link>
    <description>Recent content in representation-learning on NTU-NLP</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>ntunlp &amp;copy; 2020</copyright>
    <lastBuildDate>Thu, 09 Sep 2021 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://ntunlpsg.github.io/tags/representation-learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>A Unified Speaker Adaptation Approach for ASR</title>
      <link>https://ntunlpsg.github.io/project/asr/</link>
      <pubDate>Thu, 09 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>https://ntunlpsg.github.io/project/asr/</guid>
      <description>A unified speaker adaptation approach consisting of feature adaptation and model adaptation for ASR.</description>
    </item>
    
    <item>
      <title>UXLA: A Robust Unsupervised Data Augmentation Framework for Zero-Resouce Cross-Lingual NLP</title>
      <link>https://ntunlpsg.github.io/project/uxla/</link>
      <pubDate>Wed, 28 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://ntunlpsg.github.io/project/uxla/</guid>
      <description>We propose UXLA, a novel data augmentation framework for self-supervised learning in zero-resource transfer learning scenarios.</description>
    </item>
    
    <item>
      <title>Evaluating Pronominal Anaphora in Machine Translation: An Evaluation Measure and a Test-suite</title>
      <link>https://ntunlpsg.github.io/project/discomt/eval-anaphora/</link>
      <pubDate>Wed, 30 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>https://ntunlpsg.github.io/project/discomt/eval-anaphora/</guid>
      <description>An extensive, targeted dataset that can be used as a test suite for pronoun translation, covering multiple source languages and different pronoun errors drawn from real system translations, for English</description>
    </item>
    
    <item>
      <title>Discourse-informed Sen2Vec</title>
      <link>https://ntunlpsg.github.io/project/discourse-info-sen2vec/</link>
      <pubDate>Thu, 28 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>https://ntunlpsg.github.io/project/discourse-info-sen2vec/</guid>
      <description>CON-S2V: A Generic Framework for Incorporating</description>
    </item>
    
    <item>
      <title>Recurrent Neural Models for Fine-grained Opinion Analysis</title>
      <link>https://ntunlpsg.github.io/project/opinion-analysis/</link>
      <pubDate>Thu, 28 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>https://ntunlpsg.github.io/project/opinion-analysis/</guid>
      <description>Publications Pengfei Liu, Shafiq Joty, Helen Meng. Fine-grained Opinion Mining with Recurrent Neural Networks and Word Embeddings. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP-2015), Lisbon, Portugal, 2015.
@InProceedings{liu-joty-meng-emnlp-15, author = {Liu, Pengfei and Joty, Shafiq and Meng, Helen}, title = {Fine-grained Opinion Mining with Recurrent Neural Networks and Word Embeddings}, booktitle = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing}, year = {2015}, address = {Lisbon, Portugal}, series = {EMNLP&#39;15}, pages = {1433--1443}, url = {http://aclweb.</description>
    </item>
    
    <item>
      <title>SegBot: A Generic Neural Text Segmentation Model with Pointer Network</title>
      <link>https://ntunlpsg.github.io/project/segbot/</link>
      <pubDate>Mon, 18 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>https://ntunlpsg.github.io/project/segbot/</guid>
      <description>Online Demo Figure 1 shows the model architecture of SegBot. For EDU segmentation, the units in the input $ U0 \ to \ U8 $ are words in a sentence. Formally, given an input sequence $ U = (U_1, U_2, &amp;hellip; , U_N) $ of length $N$, we get its distributed representations $ X = (x_1, x_2, &amp;hellip; , x_N $ by looking up the corresponding embedding matrix, where $x_n \in R^k$ is the representation for the unit $U_n$ with $K$ being the dimensions.</description>
    </item>
    
  </channel>
</rss>