<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Seq2seq on NTU-NLP</title>
    <link>https://ntunlpsg.github.io/tags/seq2seq/</link>
    <description>Recent content in Seq2seq on NTU-NLP</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>ntunlp &amp;copy; 2020</copyright>
    <lastBuildDate>Sat, 01 May 2021 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://ntunlpsg.github.io/tags/seq2seq/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>RST Parsing from Scratch</title>
      <link>https://ntunlpsg.github.io/project/naacl21-rst-parsing-resource/naacl21-rst-parsing-resource/</link>
      <pubDate>Sat, 01 May 2021 00:00:00 +0000</pubDate>
      
      <guid>https://ntunlpsg.github.io/project/naacl21-rst-parsing-resource/naacl21-rst-parsing-resource/</guid>
      <description>RST Parsing from Scratch This repository contains the source code of our paper RST Parsing from Scratch in NAACL 2021.
Requirements  python: 3.7 pytorch: 1.4 transformers: 3.0  Usage To train a discourse parser:
./*_train.sh  To predict discourse tree:
./*_predict.sh  Data Format  For end-to-end parsing from scratch (no sentence guidance): we need to create the data with dummy edu_break and doc_structure. Refer to create_sample_dummy_format_data.py and dummy_format_data/sample_rawtext_data_format For other parsing models: Refer to create_sample_dummy_format_data.</description>
    </item>
    
  </channel>
</rss>